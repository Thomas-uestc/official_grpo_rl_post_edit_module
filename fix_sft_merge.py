#!/usr/bin/env python3
"""
ä¿®å¤SFT AdaLoRAæ¨¡å‹åˆå¹¶ - å¤„ç†è·¯å¾„ä¸åŒ¹é…é—®é¢˜
SFTè®­ç»ƒæ—¶çš„rank_patternä½¿ç”¨ model.layers.X ä½†å®é™…æ¨¡å‹ç»“æ„æ˜¯ model.language_model.layers.X
"""

import torch
from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor
import json
import os
from collections import OrderedDict

def fix_sft_merge():
    """ä¿®å¤SFTæ¨¡å‹åˆå¹¶ï¼Œå¤„ç†è·¯å¾„ä¸åŒ¹é…é—®é¢˜"""
    
    # é…ç½®è·¯å¾„
    base_model_path = "/data2/yixuan/.cache/huggingface/hub/Qwen-Qwen2.5-VL-7B-Instruct"
    adalora_path = "/data2/yixuan/Temporary/Qwen2.5-VL/qwen-vl-finetune/result_cvpr/hyperparameter_search_20250914_124104/hps_13_0915_0037_bs4_lr3e-05_lrcos_ga4_r16-8_t0.2-0.7_full/checkpoint-4900"
    output_path = "/data2/yixuan/Temporary/EasyR1/models/qwen2.5-vl-7b-sft-merged-fixed"
    
    print("=" * 80)
    print("ğŸ”§ ä¿®å¤SFT AdaLoRAæ¨¡å‹åˆå¹¶")
    print("=" * 80)
    print(f"ğŸ“¥ åŸºç¡€æ¨¡å‹: {base_model_path}")
    print(f"ğŸ“¥ SFTæ¨¡å‹: {adalora_path}")
    print(f"ğŸ“¤ è¾“å‡ºè·¯å¾„: {output_path}")
    
    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_path, exist_ok=True)
        
        # 1. åŠ è½½åŸºç¡€æ¨¡å‹
        print("\nğŸ“¥ åŠ è½½åŸºç¡€æ¨¡å‹...")
        base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            base_model_path,
            torch_dtype=torch.bfloat16,
            device_map="cpu",
            low_cpu_mem_usage=True,
        )
        print("âœ… åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # 2. è¯»å–AdaLoRAæƒé‡æ–‡ä»¶
        print("\nğŸ“¥ è¯»å–AdaLoRAæƒé‡...")
        adapter_weights_path = os.path.join(adalora_path, "adapter_model.safetensors")
        
        if not os.path.exists(adapter_weights_path):
            raise FileNotFoundError(f"AdaLoRAæƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {adapter_weights_path}")
        
        # ä½¿ç”¨safetensorsåŠ è½½æƒé‡
        from safetensors.torch import load_file
        adalora_weights = load_file(adapter_weights_path)
        
        print(f"âœ… AdaLoRAæƒé‡åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(adalora_weights)} ä¸ªå‚æ•°")
        
        # 3. åˆ†ææƒé‡æ˜ å°„å…³ç³»
        print("\nğŸ” åˆ†ææƒé‡æ˜ å°„å…³ç³»...")
        
        # è·å–åŸºç¡€æ¨¡å‹çš„å‚æ•°å
        base_param_names = set(name for name, _ in base_model.named_parameters())
        
        # åˆ†æAdaLoRAæƒé‡çš„è·¯å¾„æ¨¡å¼
        adalora_param_names = set(adalora_weights.keys())
        
        # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹
        print("ğŸ“‹ AdaLoRAæƒé‡ç¤ºä¾‹:")
        for i, name in enumerate(sorted(adalora_param_names)):
            if i < 5:
                print(f"   - {name}: {adalora_weights[name].shape}")
        
        print("ğŸ“‹ åŸºç¡€æ¨¡å‹å‚æ•°ç¤ºä¾‹ (language_modeléƒ¨åˆ†):")
        lang_model_params = [name for name in sorted(base_param_names) if 'language_model.layers' in name][:5]
        for name in lang_model_params:
            param = dict(base_model.named_parameters())[name]
            print(f"   - {name}: {param.shape}")
        
        # 4. åˆ›å»ºè·¯å¾„æ˜ å°„è§„åˆ™
        print("\nğŸ”„ åˆ›å»ºè·¯å¾„æ˜ å°„è§„åˆ™...")
        
        def map_adalora_to_base_path(adalora_name):
            """å°†AdaLoRAæƒé‡è·¯å¾„æ˜ å°„åˆ°åŸºç¡€æ¨¡å‹è·¯å¾„"""
            # AdaLoRAä½¿ç”¨: base_model.model.layers.X.xxx
            # å®é™…æ¨¡å‹: model.language_model.layers.X.xxx
            
            if adalora_name.startswith('base_model.'):
                # ç§»é™¤ 'base_model.' å‰ç¼€
                mapped_name = adalora_name[11:]  # len('base_model.') = 11
                
                # å°† 'model.layers' æ›¿æ¢ä¸º 'model.language_model.layers'
                if mapped_name.startswith('model.layers.'):
                    mapped_name = mapped_name.replace('model.layers.', 'model.language_model.layers.')
                
                return mapped_name
            
            return adalora_name
        
        # 5. åº”ç”¨LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹
        print("\nâš¡ åº”ç”¨LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹...")
        
        applied_count = 0
        skipped_count = 0
        
        # å°†åŸºç¡€æ¨¡å‹å‚æ•°è½¬æ¢ä¸ºå­—å…¸ä»¥ä¾¿ä¿®æ”¹
        base_state_dict = dict(base_model.named_parameters())
        
        for adalora_name, adalora_weight in adalora_weights.items():
            # è·³è¿‡éæƒé‡å‚æ•°
            if not adalora_name.endswith(('.weight', '.bias')):
                continue
                
            # è§£æLoRAå‚æ•°
            if '.lora_A.' in adalora_name or '.lora_B.' in adalora_name or '.lora_E.' in adalora_name:
                # è¿™æ˜¯LoRAçš„Aã€Bã€EçŸ©é˜µï¼Œéœ€è¦é‡æ„åŸå§‹æƒé‡
                
                # è·å–åŸºç¡€å‚æ•°å (ç§»é™¤loraç›¸å…³åç¼€)
                if '.lora_A.' in adalora_name:
                    base_param_name = adalora_name.replace('.lora_A.', '.').replace('base_model.', '')
                elif '.lora_B.' in adalora_name:
                    base_param_name = adalora_name.replace('.lora_B.', '.').replace('base_model.', '')
                elif '.lora_E.' in adalora_name:
                    continue  # EçŸ©é˜µæ˜¯AdaLoRAç‰¹æœ‰çš„ï¼Œæš‚æ—¶è·³è¿‡
                else:
                    continue
                
                # æ˜ å°„åˆ°å®é™…æ¨¡å‹è·¯å¾„
                mapped_param_name = map_adalora_to_base_path('base_model.' + base_param_name)
                
                if mapped_param_name in base_state_dict:
                    # è¿™é‡Œéœ€è¦å®ç°å®Œæ•´çš„AdaLoRAæƒé‡é‡æ„ï¼Œä½†è¿™å¾ˆå¤æ‚
                    # æš‚æ—¶å…ˆè·³è¿‡ï¼Œä½¿ç”¨æ›´ç®€å•çš„æ–¹æ³•
                    skipped_count += 1
                    continue
            else:
                # ç›´æ¥æƒé‡å‚æ•°
                mapped_param_name = map_adalora_to_base_path(adalora_name)
                
                if mapped_param_name in base_state_dict:
                    # ç›´æ¥æ›¿æ¢æƒé‡
                    with torch.no_grad():
                        base_state_dict[mapped_param_name].copy_(adalora_weight.to(base_state_dict[mapped_param_name].device))
                    applied_count += 1
                else:
                    print(f"âš ï¸  æœªæ‰¾åˆ°åŒ¹é…çš„åŸºç¡€å‚æ•°: {mapped_param_name}")
                    skipped_count += 1
        
        print(f"ğŸ“Š æƒé‡åº”ç”¨ç»Ÿè®¡:")
        print(f"   - å·²åº”ç”¨: {applied_count}")
        print(f"   - å·²è·³è¿‡: {skipped_count}")
        
        # 6. æ‰‹åŠ¨é‡æ„AdaLoRAæƒé‡ (ç®€åŒ–ç‰ˆæœ¬)
        print("\nğŸ”§ æ‰‹åŠ¨é‡æ„AdaLoRAæƒé‡...")
        
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„æ–¹æ³•ï¼šç›´æ¥æ£€æŸ¥æ˜¯å¦æœ‰é¢„è®¡ç®—çš„åˆå¹¶æƒé‡
        # åœ¨æŸäº›AdaLoRAå®ç°ä¸­ï¼Œå¯èƒ½ä¼šä¿å­˜åˆå¹¶åçš„æƒé‡
        
        # æŸ¥æ‰¾å¯èƒ½çš„åˆå¹¶æƒé‡æ–‡ä»¶
        merged_weights_candidates = [
            os.path.join(adalora_path, "pytorch_model.bin"),
            os.path.join(adalora_path, "model.safetensors"),
            os.path.join(adalora_path, "merged_model.safetensors"),
        ]
        
        found_merged = False
        for candidate in merged_weights_candidates:
            if os.path.exists(candidate):
                print(f"ğŸ¯ å‘ç°å¯èƒ½çš„åˆå¹¶æƒé‡æ–‡ä»¶: {candidate}")
                found_merged = True
                break
        
        if not found_merged:
            print("âš ï¸  æœªå‘ç°é¢„åˆå¹¶æƒé‡æ–‡ä»¶ï¼Œéœ€è¦æ‰‹åŠ¨é‡æ„AdaLoRA")
            print("ğŸ’¡ å»ºè®®ï¼šä½¿ç”¨åŸå§‹SFTè®­ç»ƒè„šæœ¬é‡æ–°ä¿å­˜åˆå¹¶åçš„æ¨¡å‹")
        
        # 7. ä¿å­˜ä¿®å¤åçš„æ¨¡å‹
        print(f"\nğŸ’¾ ä¿å­˜ä¿®å¤åçš„æ¨¡å‹åˆ°: {output_path}")
        base_model.save_pretrained(output_path)
        
        # ä¿å­˜tokenizerå’Œprocessor
        print("ğŸ“‹ ä¿å­˜tokenizerå’Œprocessor...")
        tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
        tokenizer.save_pretrained(output_path)
        
        processor = AutoProcessor.from_pretrained(base_model_path, trust_remote_code=True)
        processor.save_pretrained(output_path)
        
        print("âœ… ä¿®å¤åçš„æ¨¡å‹ä¿å­˜å®Œæˆ")
        
        # 8. éªŒè¯ä¿®å¤ç»“æœ
        print(f"\nğŸ” éªŒè¯ä¿®å¤ç»“æœ...")
        
        # é‡æ–°åŠ è½½éªŒè¯
        test_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            output_path,
            torch_dtype=torch.bfloat16,
            device_map="cpu",
            low_cpu_mem_usage=True,
        )
        
        # å¯¹æ¯”ä¸€ä¸ªå‚æ•°çœ‹æ˜¯å¦æœ‰å˜åŒ–
        test_param_name = "model.language_model.layers.0.self_attn.q_proj.weight"
        original_param = dict(base_model.named_parameters())[test_param_name]
        fixed_param = dict(test_model.named_parameters())[test_param_name]
        
        param_diff = torch.abs(original_param - fixed_param).max().item()
        print(f"ğŸ“Š å‚æ•°å·®å¼‚æ£€æŸ¥:")
        print(f"   - æµ‹è¯•å‚æ•°: {test_param_name}")
        print(f"   - æœ€å¤§å·®å¼‚: {param_diff:.2e}")
        
        if param_diff > 1e-6:
            print("âœ… æ£€æµ‹åˆ°å‚æ•°å˜åŒ–ï¼Œä¿®å¤å¯èƒ½æˆåŠŸ")
        else:
            print("âš ï¸  æœªæ£€æµ‹åˆ°å‚æ•°å˜åŒ–ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ ä¿®å¤å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    success = fix_sft_merge()
    
    if success:
        print(f"\n" + "=" * 80)
        print("ğŸ‰ SFTæ¨¡å‹ä¿®å¤å®Œæˆï¼")
        print("ğŸ“ ä¿®å¤åæ¨¡å‹è·¯å¾„: /data2/yixuan/Temporary/EasyR1/models/qwen2.5-vl-7b-sft-merged-fixed")
        print("ğŸ’¡ æ³¨æ„ï¼šç”±äºAdaLoRAæƒé‡é‡æ„çš„å¤æ‚æ€§ï¼Œå»ºè®®è¿›ä¸€æ­¥éªŒè¯æ¨¡å‹æ•ˆæœ")
        print("=" * 80)
    else:
        print(f"\nâŒ ä¿®å¤å¤±è´¥")
    
    return success

if __name__ == "__main__":
    import sys
    success = main()
    sys.exit(0 if success else 1)
