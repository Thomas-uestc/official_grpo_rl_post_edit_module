#!/usr/bin/env python3
"""
é¢„å…ˆåˆå¹¶SFT AdaLoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹
ä¸“é—¨é€‚é…ç”¨æˆ·çš„AdaLoRAé…ç½®ï¼šinit_r=16, target_r=8, fullæ¨¡å¼
"""

import torch
from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor
from peft import PeftModel, get_peft_model, AdaLoraConfig
import os
import json
import re
from pathlib import Path

def merge_sft_model():
    """åˆå¹¶SFTæ¨¡å‹æƒé‡"""
    # æ ¹æ®ç”¨æˆ·å®é™…è·¯å¾„é…ç½®
    base_model_path = "/data2/yixuan/.cache/huggingface/hub/Qwen-Qwen2.5-VL-7B-Instruct"
    adalora_path = "/data2/yixuan/Temporary/Qwen2.5-VL/qwen-vl-finetune/result_cvpr/hps7b_long_cot_v2/hps7b_long_cot_22_1030_0950_bs4_lr1e-04_lrcos_ga4_r64-4_t0.2-0.7_qkv/checkpoint-3600"
    output_path = "/data2/yixuan/Temporary/EasyR1/models/qwen2.5-vl-7b-sft-merged-v2-long-cot"
    
    print("ğŸš€ å¼€å§‹åˆå¹¶SFT AdaLoRAæ¨¡å‹...")
    print(f"ğŸ“¥ åŸºç¡€æ¨¡å‹: {base_model_path}")
    print(f"ğŸ“¥ SFTæ¨¡å‹: {adalora_path}")
    print(f"ğŸ“¤ è¾“å‡ºè·¯å¾„: {output_path}")
    
    try:
        # éªŒè¯è·¯å¾„å­˜åœ¨
        if not os.path.exists(base_model_path):
            raise FileNotFoundError(f"åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {base_model_path}")
        if not os.path.exists(adalora_path):
            raise FileNotFoundError(f"AdaLoRA checkpointè·¯å¾„ä¸å­˜åœ¨: {adalora_path}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_path, exist_ok=True)
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        print("ğŸ“¥ åŠ è½½åŸºç¡€æ¨¡å‹...")
        base_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            base_model_path,
            torch_dtype=torch.bfloat16,
            device_map="cpu",
            low_cpu_mem_usage=True,
            attn_implementation="flash_attention_2"  # ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´
        )
        print("âœ… åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # è¯»å–AdaLoRAé…ç½®
        print("ğŸ“‹ è¯»å–AdaLoRAé…ç½®...")
        adapter_config_path = os.path.join(adalora_path, "adapter_config.json")
        with open(adapter_config_path, 'r') as f:
            adapter_config = json.load(f)
        
        print(f"ğŸ“‹ AdaLoRAé…ç½®:")
        print(f"   - PEFTç±»å‹: {adapter_config.get('peft_type')}")
        print(f"   - å½“å‰rank: {adapter_config.get('r')}")
        print(f"   - åˆå§‹rank: {adapter_config.get('init_r')}")
        print(f"   - ç›®æ ‡rank: {adapter_config.get('target_r')}")
        print(f"   - Alpha: {adapter_config.get('lora_alpha')}")
        print(f"   - Targetæ¨¡å—: {adapter_config.get('target_modules')}")
        print(f"   - è®­ç»ƒæ­¥æ•°: {adapter_config.get('total_step')}")
        print(f"   - tinit: {adapter_config.get('tinit')}, tfinal: {adapter_config.get('tfinal')}")
        
        # ç›´æ¥ä»checkpointåŠ è½½PEFTæ¨¡å‹ï¼ˆä¸é‡å»ºé…ç½®ï¼‰
        print("ğŸ”§ ç›´æ¥ä»checkpointåŠ è½½PEFTæ¨¡å‹...")
        peft_model = PeftModel.from_pretrained(
            base_model, 
            adalora_path, 
            adapter_name="default",
            is_trainable=False  # è®¾ç½®ä¸ºä¸å¯è®­ç»ƒï¼Œå› ä¸ºæˆ‘ä»¬åªæ˜¯è¦åˆå¹¶
        )
        print("âœ… PEFTæ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æ˜¾ç¤ºå¯è®­ç»ƒå‚æ•°ç»Ÿè®¡
        print("ğŸ“Š PEFTæ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        peft_model.print_trainable_parameters()
        
        # åˆå¹¶æƒé‡
        print("ğŸ”„ åˆå¹¶æƒé‡...")
        merged_model = peft_model.merge_and_unload()
        print("âœ… æƒé‡åˆå¹¶å®Œæˆ")
        
        # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
        print("ğŸ’¾ ä¿å­˜åˆå¹¶åçš„æ¨¡å‹...")
        merged_model.save_pretrained(
            output_path,
            safe_serialization=True,
            max_shard_size="5GB"
        )
        print("âœ… æ¨¡å‹ä¿å­˜å®Œæˆ")
        
        # å¤åˆ¶tokenizerå’Œprocessor
        print("ğŸ“‹ å¤åˆ¶tokenizerå’Œprocessor...")
        tokenizer = AutoTokenizer.from_pretrained(base_model_path)
        tokenizer.save_pretrained(output_path)
        
        processor = AutoProcessor.from_pretrained(base_model_path)
        processor.save_pretrained(output_path)
        print("âœ… Tokenizerå’Œprocessorä¿å­˜å®Œæˆ")
        
        # éªŒè¯åˆå¹¶åçš„æ¨¡å‹
        print("ğŸ” éªŒè¯åˆå¹¶åçš„æ¨¡å‹...")
        test_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            output_path,
            torch_dtype=torch.bfloat16,
            device_map="cpu",
            low_cpu_mem_usage=True,
        )
        
        total_params = sum(p.numel() for p in test_model.parameters())
        trainable_params = sum(p.numel() for p in test_model.parameters() if p.requires_grad)
        
        print(f"ğŸ“Š åˆå¹¶åæ¨¡å‹å‚æ•°:")
        print(f"   - æ€»å‚æ•°: {total_params:,} ({total_params/1e9:.2f}B)")
        print(f"   - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰PEFTé…ç½®æ®‹ç•™
        has_peft = hasattr(test_model, 'peft_config')
        print(f"ğŸ¯ PEFTé…ç½®æ®‹ç•™: {has_peft}")
        
        if not has_peft:
            print("ğŸ‰ åˆå¹¶æˆåŠŸï¼æ¨¡å‹å·²è½¬æ¢ä¸ºæ ‡å‡†transformer")
        else:
            print("âš ï¸ è­¦å‘Šï¼šæ¨¡å‹ä»åŒ…å«PEFTé…ç½®")
        
        # åˆ›å»ºä½¿ç”¨è¯´æ˜æ–‡ä»¶
        from datetime import datetime
        readme_content = f"""# Merged SFT Model

## æ¨¡å‹ä¿¡æ¯
- åŸºç¡€æ¨¡å‹: Qwen2.5-VL-7B-Instruct
- SFTæ–¹æ³•: AdaLoRA (init_r=16, target_r=8)
- è®­ç»ƒæ­¥æ•°: {adapter_config.get('total_step', 'N/A')}
- åˆå¹¶æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## åœ¨RLè®­ç»ƒä¸­ä½¿ç”¨
åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®ï¼š
```yaml
worker:
  actor:
    model:
      model_path: {output_path}
      use_peft: false
```

## å‚æ•°ç»Ÿè®¡
- æ€»å‚æ•°: {total_params:,} ({total_params/1e9:.2f}B)
- å¯è®­ç»ƒå‚æ•°: {trainable_params:,}
"""
        
        with open(os.path.join(output_path, "README.md"), "w") as f:
            f.write(readme_content)
        
        print(f"\nâœ… åˆå¹¶å®Œæˆï¼")
        print(f"ğŸ“ åˆå¹¶åæ¨¡å‹è·¯å¾„: {output_path}")
        print(f"ğŸ’¡ ç°åœ¨å¯ä»¥åœ¨RLè®­ç»ƒä¸­ä½¿ç”¨æ­¤è·¯å¾„ï¼Œæ— éœ€PEFTé…ç½®")
        
        return output_path
        
    except Exception as e:
        print(f"âŒ åˆå¹¶å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸš€ SFT AdaLoRAæ¨¡å‹åˆå¹¶å·¥å…·")
    print("ğŸ¯ ä¸“é—¨é€‚é… Qwen2.5-VL + AdaLoRA (init_r=16, target_r=8)")
    print("=" * 80)
    
    merged_path = merge_sft_model()
    
    if merged_path:
        print(f"\n" + "=" * 80)
        print(f"ğŸ‰ åˆå¹¶æˆåŠŸï¼")
        print(f"ğŸ“ åˆå¹¶åæ¨¡å‹: {merged_path}")
        print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•ï¼š")
        print(f"   åœ¨RLè®­ç»ƒé…ç½®ä¸­è®¾ç½®ï¼š")
        print(f"   worker.actor.model.model_path: {merged_path}")
        print(f"   worker.actor.model.use_peft: false")
        print(f"\nğŸ”§ VERLè®­ç»ƒå‘½ä»¤ç¤ºä¾‹ï¼š")
        print(f"   python verl/trainer/main.py config.yaml")
        print(f"=" * 80)
        return True
    else:
        print(f"\nâŒ åˆå¹¶å¤±è´¥")
        return False

if __name__ == "__main__":
    import sys
    success = main()
    sys.exit(0 if success else 1)
