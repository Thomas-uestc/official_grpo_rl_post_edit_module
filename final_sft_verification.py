#!/usr/bin/env python3
"""
æœ€ç»ˆSFTçŸ¥è¯†éªŒè¯ - ä¸“é—¨æµ‹è¯•å›¾åƒç¼–è¾‘ç›¸å…³èƒ½åŠ›
"""

import torch
from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor
import json
import os
from PIL import Image
import io
import base64

def create_test_image():
    """åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•å›¾åƒ"""
    from PIL import Image, ImageDraw
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•å›¾åƒ
    img = Image.new('RGB', (224, 224), color='white')
    draw = ImageDraw.Draw(img)
    
    # ç»˜åˆ¶ä¸€ä¸ªç®€å•çš„åœºæ™¯ï¼šè“è‰²å¤©ç©ºï¼Œç»¿è‰²è‰åœ°ï¼Œçº¢è‰²æˆ¿å­
    draw.rectangle([0, 0, 224, 112], fill='lightblue')  # å¤©ç©º
    draw.rectangle([0, 112, 224, 224], fill='lightgreen')  # è‰åœ°
    draw.rectangle([80, 60, 144, 120], fill='red')  # æˆ¿å­
    draw.rectangle([106, 90, 118, 120], fill='brown')  # é—¨
    
    return img

def image_to_base64(image):
    """å°†PILå›¾åƒè½¬æ¢ä¸ºbase64å­—ç¬¦ä¸²"""
    buffer = io.BytesIO()
    image.save(buffer, format='PNG')
    img_str = base64.b64encode(buffer.getvalue()).decode()
    return img_str

def test_image_editing_capability(model, tokenizer, processor, model_name="æ¨¡å‹"):
    """æµ‹è¯•å›¾åƒç¼–è¾‘ç›¸å…³çš„èƒ½åŠ›"""
    print(f"\nğŸ§ª æµ‹è¯•{model_name}çš„å›¾åƒç¼–è¾‘èƒ½åŠ›...")
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    test_image = create_test_image()
    
    # å›¾åƒç¼–è¾‘ç›¸å…³çš„æµ‹è¯•æç¤º
    test_prompts = [
        "Describe what you see in this image.",
        "What objects are in this image?",
        "What colors do you see in this image?",
        "How would you edit this image to make it more beautiful?",
        "What would you change about this image?",
        "Describe the composition of this image.",
        "What is the main subject of this image?",
        "How would you improve the lighting in this image?",
        "What editing suggestions do you have for this image?",
        "Describe the scene in this image in detail.",
    ]
    
    responses = []
    
    for i, prompt in enumerate(test_prompts):
        try:
            # å‡†å¤‡è¾“å…¥
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": test_image},
                        {"type": "text", "text": prompt}
                    ]
                }
            ]
            
            # å¤„ç†è¾“å…¥
            text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            image_inputs, video_inputs = process_vision_info(messages)
            inputs = processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt"
            )
            
            # ç”Ÿæˆå“åº”
            with torch.no_grad():
                generated_ids = model.generate(
                    **inputs,
                    max_new_tokens=128,
                    do_sample=False,
                    temperature=0.1,
                )
            
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            
            response = processor.batch_decode(
                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
            )[0]
            
            responses.append({
                'prompt': prompt,
                'response': response.strip()
            })
            
            print(f"   æµ‹è¯• {i+1}/10: {prompt[:30]}...")
            
        except Exception as e:
            print(f"   âŒ æµ‹è¯• {i+1} å¤±è´¥: {e}")
            responses.append({
                'prompt': prompt,
                'response': f"ERROR: {str(e)}"
            })
    
    return responses

def process_vision_info(messages):
    """å¤„ç†è§†è§‰ä¿¡æ¯ - ä»Qwen2.5-VLç¤ºä¾‹ä»£ç æ”¹ç¼–"""
    image_inputs = []
    video_inputs = []
    
    for message in messages:
        if isinstance(message, dict) and "content" in message:
            for content in message["content"]:
                if content.get("type") == "image":
                    image_inputs.append(content["image"])
                elif content.get("type") == "video":
                    video_inputs.append(content["video"])
    
    return image_inputs, video_inputs

def compare_model_responses(original_responses, merged_responses):
    """å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„å“åº”"""
    print(f"\nğŸ“Š å¯¹æ¯”æ¨¡å‹å“åº”...")
    
    different_responses = 0
    similar_responses = 0
    
    for i, (orig, merged) in enumerate(zip(original_responses, merged_responses)):
        orig_resp = orig['response'].lower()
        merged_resp = merged['response'].lower()
        
        # ç®€å•çš„ç›¸ä¼¼åº¦æ£€æŸ¥
        if orig_resp != merged_resp:
            different_responses += 1
            print(f"\n   ğŸ“ æµ‹è¯• {i+1}: {orig['prompt'][:40]}...")
            print(f"      åŸå§‹: {orig_resp[:100]}{'...' if len(orig_resp) > 100 else ''}")
            print(f"      åˆå¹¶: {merged_resp[:100]}{'...' if len(merged_resp) > 100 else ''}")
            print("      âœ… å“åº”æœ‰å·®å¼‚")
        else:
            similar_responses += 1
    
    print(f"\n   ğŸ“Š å“åº”å¯¹æ¯”ç»Ÿè®¡:")
    print(f"      - æœ‰å·®å¼‚: {different_responses}")
    print(f"      - ç›¸ä¼¼: {similar_responses}")
    print(f"      - å·®å¼‚æ¯”ä¾‹: {different_responses/(different_responses+similar_responses)*100:.1f}%")
    
    return different_responses > 0

def verify_sft_knowledge():
    """éªŒè¯SFTçŸ¥è¯†æ˜¯å¦æˆåŠŸèå…¥"""
    print("=" * 80)
    print("ğŸ” æœ€ç»ˆSFTçŸ¥è¯†éªŒè¯ - å›¾åƒç¼–è¾‘èƒ½åŠ›æµ‹è¯•")
    print("=" * 80)
    
    # è·¯å¾„é…ç½®
    original_model_path = "/data2/yixuan/.cache/huggingface/hub/Qwen-Qwen2.5-VL-7B-Instruct"
    merged_model_path = "/data2/yixuan/Temporary/EasyR1/models/qwen2.5-vl-7b-sft-merged"
    
    try:
        # 1. åŠ è½½åŸå§‹æ¨¡å‹
        print("\nğŸ“¥ åŠ è½½åŸå§‹æ¨¡å‹...")
        original_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            original_model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            low_cpu_mem_usage=True,
        )
        
        original_processor = AutoProcessor.from_pretrained(original_model_path, trust_remote_code=True)
        original_tokenizer = AutoTokenizer.from_pretrained(original_model_path, trust_remote_code=True)
        print("âœ… åŸå§‹æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # 2. åŠ è½½åˆå¹¶æ¨¡å‹
        print("\nğŸ“¥ åŠ è½½åˆå¹¶æ¨¡å‹...")
        merged_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            merged_model_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            low_cpu_mem_usage=True,
        )
        
        merged_processor = AutoProcessor.from_pretrained(merged_model_path, trust_remote_code=True)
        merged_tokenizer = AutoTokenizer.from_pretrained(merged_model_path, trust_remote_code=True)
        print("âœ… åˆå¹¶æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # 3. æµ‹è¯•åŸå§‹æ¨¡å‹
        original_responses = test_image_editing_capability(
            original_model, original_tokenizer, original_processor, "åŸå§‹æ¨¡å‹"
        )
        
        # 4. æµ‹è¯•åˆå¹¶æ¨¡å‹
        merged_responses = test_image_editing_capability(
            merged_model, merged_tokenizer, merged_processor, "åˆå¹¶æ¨¡å‹"
        )
        
        # 5. å¯¹æ¯”å“åº”
        has_differences = compare_model_responses(original_responses, merged_responses)
        
        # 6. æœ€ç»ˆç»“è®º
        print(f"\n" + "=" * 80)
        print("ğŸ“Š æœ€ç»ˆéªŒè¯ç»“æœ")
        print("=" * 80)
        
        if has_differences:
            print("ğŸ‰ éªŒè¯æˆåŠŸï¼")
            print("âœ… åˆå¹¶æ¨¡å‹åœ¨å›¾åƒç¼–è¾‘ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä¸åŸå§‹æ¨¡å‹çš„å·®å¼‚")
            print("âœ… è¿™è¡¨æ˜SFTè®­ç»ƒçš„çŸ¥è¯†å·²ç»æˆåŠŸèå…¥åˆ°æ¨¡å‹ä¸­")
            print("ğŸ¯ å»ºè®®ï¼šå¯ä»¥å¼€å§‹RLè®­ç»ƒ")
        else:
            print("âš ï¸  éªŒè¯ç»“æœä¸æ˜ç¡®")
            print("â“ åˆå¹¶æ¨¡å‹ä¸åŸå§‹æ¨¡å‹çš„å“åº”è¿‡äºç›¸ä¼¼")
            print("ğŸ’¡ å¯èƒ½çš„åŸå› ï¼š")
            print("   1. SFTè®­ç»ƒçš„æ”¹åŠ¨ä¸»è¦åœ¨ç‰¹å®šé¢†åŸŸï¼Œæµ‹è¯•ç”¨ä¾‹æœªè¦†ç›–")
            print("   2. åˆå¹¶è¿‡ç¨‹ç¡®å®æœªæˆåŠŸ")
            print("   3. æµ‹è¯•æ–¹æ³•éœ€è¦æ”¹è¿›")
        
        return has_differences
        
    except Exception as e:
        print(f"\nâŒ éªŒè¯è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    success = verify_sft_knowledge()
    return success

if __name__ == "__main__":
    import sys
    success = main()
    sys.exit(0 if success else 1)
